---
title: "Assignment 6: CART"
author: "Maria Sanchez"
date: "October 2024"
format: html
editor: source
self-contained: true
toc: true
toc-expand: true
toc-depth: 3
---


```{r setup, include=FALSE}
rm(list = ls()) # clears global environment
#knitr::opts_knit$set(root.dir = '/Users/mariasanchez/Documents/QTM6300')
```

# Context

Let’s take a look at the data located in ToyotaCorolla.csv. Each row represents a used car, with the collection of columns ranging from a fairly detailed model name (Model) to performance characteristics like horsepower (HP), and engine size (cc). Our goal here is to predict the numeric target Price. You can imagine several different applications for this sort of algorithm. Perhaps work for an insurance company, and are dissatisfied with the industry standard car value estimates, or perhaps you want to make money at auto auctions by bidding on cars that you think are currently significantly undervalued.

The first model that pops to mind for this type of problem is likely linear regression, and that’s not bad intuition. After all, linear regression is a simple and often powerful technique that has the potential to tell us a lot about our problem. At the same time, it, by its very nature, forces us into a very particular worldview – namely that the target and every input are related linearly. This is a bold claim, and one that is often simply not true. Trees, on the other hand, try to uncover the relationship between the most important inputs and the target, whatever form they might take.


# Question 1

To get ready for modeling, complete the following data management steps:

* Import the data and call it "df".

* Remove the variable "Model." This should have no predictive power given the subtle differences between the same brand of car.

* Convert variables to factors, if necessary. Note that predictors in CART can be either categorical or numeric.

* Partition the data using a 60-40 training-test split using random seed 1234. 

```{r}
#insert code
df <-read.csv("ToyotaCorolla.csv")
df$Model <- NULL

df$Fuel_Type <- as.factor(df$Fuel_Type)
df$Met_Color <- as.factor(df$Met_Color)
df$Automatic <- as.factor(df$Automatic)
df$Doors <- as.factor(df$Doors)

set.seed(1234)
N <- nrow(df)
trainingSize <- round(N*0.6)
trainingCases <- sample(N, trainingSize)
training <- df[trainingCases,]
test <- df[-trainingCases,]
```


# Question 2
Build your CART model using all available predictors to predict Price. Use default stopping rules. Also ask R to display the resulting tree using rpart.plot. When doing so, add digits=-2 as an option/parameter within your rpart.plot() function if you wish to remove scientific notation, such as rpart.plot(model, digits=-2).


```{r}
#insert code
library(rpart)
library(rpart.plot)

model <- rpart(Price ~ ., data=training)
rpart.plot(model, digits=-2)
```

# Question 3
In class, each node contained three numbers. In your tree for Question 2, you should only see two numbers in each node. What is missing and why do you think that is?

Answer: 


# Question 4
In examining your tree from Question 2, if a car is 72 months old and has been driven 102,000 km, what is the predicted price? Note that the Age variable is in months and KM variable is in kilometers. Price is in Euros.

Answer: The predicted price is 7949 Euros.

# Question 5
What would the price of a 50-month-old car with 20 thousand kilometers on the odometer be according to your regression tree with default stopping rules? Price is in Euros.

Answer: The predicted price is 11656 Euros.


# Question 6
Calculate the MAPE and RMSE for your model. Interpret each of them in context.
Answer: MAPE: on average, the model’s predictions are off by about 10.67% from the actual prices.
RMSE: on average, the model’s predictions deviate from the actual prices by about 1,496 Euros.
```{r}
#insert code
predictions <- predict(model, newdata = test)
observations <- test$Price
errors <- observations - predictions
mape <- mean(abs(observations-predictions)/observations)
mape
rmse <- sqrt(mean((observations-predictions)^2))
rmse
```

# Question 7
What is the benchmark MAPE and RMSE associated with this model? Is your model useful?

Answer: Yes, my model is useful because the benchmark MAPE and RMSE are much higher than the model's.

```{r}
#insert code
errors_bench <- observations - mean(training$Price)
mape_bench <- mean(abs(errors_bench)/observations)
mape_bench
rmse_bench <- sqrt(mean(errors_bench^2))
rmse_bench
```

# Question 8
You ran a model with the following parameters:

* minsplit=50,minbucket=20, cp=0.05

Your friend ran a model with the following parameters:

* minsplit=2,minbucket=1, cp=0.001

Do not run the models right now. Which model is more likely to be overfit, and why?


Answer: My friend’s model is more likely to be overfit due to the lower minsplit, minbucket, and cp values.

# Question 9
Now run the two models from Question 8 using those parameters and calculate the MAPE for both models. Do the MAPE align with your hypothesis on which model is more likely to be overfit? Why? 

Answer: The MAPE values do not fully align with our overfitting hypothesis, as Model 2 has a lower MAPE on the test set. However, Model 2’s lower minsplit, minbucket, and cp values make it more complex and likely to overfit, meaning it might perform worse on new or different data. So while Model 2 has a lower MAPE here, it’s still more prone to overfitting than Model 1.

```{r}
#insert code
stoppingRules <- rpart.control(minsplit=50,minbucket=20, cp=0.05)
model1 <- rpart(Price ~ ., data=training, control=stoppingRules)
rpart.plot(model1)
predictions <- predict(model1, newdata = test)
observations <- test$Price
errors <- observations - predictions
mape1 <- mean(abs(observations-predictions)/observations)
mape1
```
```{r}
#insert code
stoppingRules <- rpart.control(minsplit=2,minbucket=1, cp=0.001)
model2 <- rpart(Price ~ ., data=training, control=stoppingRules)
rpart.plot(model2)
predictions <- predict(model2, newdata = test)
observations <- test$Price
errors <- observations - predictions
mape2 <- mean(abs(observations-predictions)/observations)
mape2
```

# Question 10

Now, try to take your "overfitted" model and prune it. Calculate the MAPE for this pruned model and compare the MAPE to the MAPE of the "overfitted" model. What do you find? What might this say about our "overfitted" model?

Answer: The MAPE for the pruned model is the same as for the overfitted model. This means that the extra complexity in the overfitted model didn’t actually make it more accurate on the test data

```{r}
#insert code
source("BabsonAnalytics.R")
pruned <- easyPrune(model2)
rpart.plot(pruned)
pruned_predictions <- predict(pruned, newdata = test)
observations <- test$Price
errors <- observations - pruned_predictions
mape_pruned <- mean(abs(errors) / observations) 
mape_pruned
```



# Question 11
A friend of yours used a different random seed for their partition and has some confusing results. Your friend computed the MAPE for the default model and then pruned the model, finding that the two MAPE numbers matched almost exactly. Your friend has plotted both trees and confirmed that the the pruning is actually doing something, i.e., branches are actually being removed from the tree. 

Can this be possible? Or is your friend making a mistake somewhere in the MAPE computation? Make a concrete argument one way or the other. You may cite specific properties of model performance, overfitting properties, or something else in order to make your case. 

Answer: Yes, it’s possible for the MAPE of the default and pruned models to be almost the same. It can mean that the branches removed during pruning weren’t adding much predictive power. Pruning simplified the model without hurting its accuracy, so my friend’s results make sense and don’t indicate an error in MAPE calculation.

